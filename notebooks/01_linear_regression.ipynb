{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 1st Course: Hand-made Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The purpose of this notebook is to get acquainted with **tensors** and **gradients**. As a guiding toy example, we shall implement linear regression using gradient descent.\n",
    "\n",
    "Let us start with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Step 1: Tensors, and Creating our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "TensorFlow **tensors** are very similar to Numpy **arrays** --- they have a certain **rank** and **shape**, the values in a tensor all have the same specifided **dtype** (which can not be `object`, though), and many array operations known from Numpy have an analogue in TensorFlow.\n",
    "To get started with our linear regression example, let's\n",
    "\n",
    "- fix some initial parameters $\\alpha$, $\\beta$,\n",
    "- fix the sample size $n$,\n",
    "- choose uniformly spaced $x_1,\\ldots,x_n$ in the intervall $[0,1]$ and\n",
    "- choose samples $y_i = \\alpha x_i + \\beta + e_i$ with some Gaussian noise $e_i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha = tf.constant(0.5)\n",
    "beta = tf.constant(1.)\n",
    "alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n = 21\n",
    "x = tf.linspace(0., 1., n)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y = alpha * x + beta + tf.random.normal((n,), stddev=0.1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's plot the dataset including the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y) # the points\n",
    "plt.plot([0, 1], [beta, alpha + beta], color='orange') # the line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Step 2: Variables, and Initializing our Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A TensorFlow **variable** is a wrapper for a tensor and\n",
    "\n",
    "> the best way to represent shared, persistent state manipulated by your program.\n",
    "\n",
    "They should be used to store **model parameters** and receive special treatment when tracking gradients and optimizing or saving models.\n",
    "\n",
    "For our linear regression example, we want to start with random parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha = tf.Variable(tf.random.uniform((1,),-5,5)[0], name=\"alpha\")\n",
    "beta = tf.Variable(tf.random.uniform((1,),-5,5)[0], name=\"beta\")\n",
    "alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To update the value of a TensorFlow variable, one has to use the `assign` method or variants like `assing_add`, `assign_sub` and so on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Step 3: Gradients and Gradient Tapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To compute **gradients** of a function with respect TensorFlow tensors or variables, use the class `tf.GradientTape` and its methods `watch` and `gradient` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = alpha * x + beta\n",
    "tape.gradient(y, [alpha, x, beta])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A gradient tape records the gradients\n",
    "\n",
    "- of the functions computed in its scope\n",
    "- with respect to `tf.Variable` instances and with respect to tensors that are watched explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Step 4: Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To perform gradient descent, we need to compute the gradients of our **loss** with respect to the **model parameters**. Let us first write down the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(alpha, beta, x, y):\n",
    "    y_predict = alpha * x + beta\n",
    "    errors = y - y_predict\n",
    "    loss = tf.reduce_sum(errors * errors)\n",
    "    return loss\n",
    "\n",
    "compute_loss(alpha, beta, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This could be shortened using suitable tensor operations (see the exercises).\n",
    "\n",
    "Let us turn to the gradient descent step.\n",
    "\n",
    "In the function above, the loss is computed from the parameters and the samples using several elementary functions. To compute the gradient of the loss with respect to the parameters, we need to\n",
    "\n",
    "- keep track of the gradients of the elementary functions and\n",
    "- combine these one-step gradients using the chain rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_step(alpha, beta, x, y, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(alpha, beta, x, y)\n",
    "        grad_alpha, grad_beta = tape.gradient(loss, [alpha, beta])\n",
    "    alpha.assign_sub(grad_alpha * learning_rate)\n",
    "    beta.assign_sub(grad_beta * learning_rate)\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(alpha, beta, x, y, nr_steps=10, learning_rate=0.01):\n",
    "    alphas, betas, losses = [], [], []\n",
    "    for step in range(0, nr_steps):\n",
    "        loss = gradient_step(alpha, beta, x, y, learning_rate)\n",
    "        alphas.append(alpha.read_value())\n",
    "        betas.append(beta.read_value())\n",
    "        losses.append(loss)\n",
    "    return tf.stack(alphas), tf.stack(betas), tf.stack(losses)\n",
    "\n",
    "def choose_params():\n",
    "    alpha = tf.Variable(tf.random.uniform((1,),-5,5)[0], name=\"alpha\")\n",
    "    beta = tf.Variable(tf.random.uniform((1,),-5,5)[0], name=\"beta\")\n",
    "    return alpha, beta\n",
    "\n",
    "alphas, betas, losses = gradient_descent(*choose_params(), x, y)\n",
    "alphas, betas, losses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let us visualize the results quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def visualize(alphas, betas, losses):\n",
    "    pd.Series(losses, name='loss').plot()\n",
    "    pd.DataFrame({'alpha': alphas, 'beta': betas}).plot()\n",
    "    \n",
    "visualize(*gradient_descent(*choose_params(), x, y, nr_steps=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Visualizing training with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For long-running computations, TensorFlow offers a convenient tool to log and visualize data: TensorBoard. To log the data, use the `tf.summary` module as, for example, in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "LOGDIR = 'tmp'\n",
    "\n",
    "def tb_gradient_descent(alpha, beta, x, y, nr_steps=10, learning_rate=0.01):\n",
    "    path = os.path.join(LOGDIR, time.strftime('%H-%M-%S'))\n",
    "    with tf.summary.create_file_writer(path).as_default():\n",
    "        for step in range(0, nr_steps):\n",
    "            loss = gradient_step(alpha, beta, x, y, learning_rate)\n",
    "            tf.summary.scalar('alpha', alpha.read_value(), step=step)\n",
    "            tf.summary.scalar('beta', beta.read_value(), step=step)\n",
    "            tf.summary.scalar('loss', loss, step=step)\n",
    "\n",
    "tb_gradient_descent(*choose_params(), x, y, nr_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=$LOGDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Eager mode versus graph mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "By default, TensorFlow 2 performs all tensor operations eagerly, which helps debugging and prototyping. But if we decorate a tensor function with `@tf.function`, on first run, the function gets compiled to a computation graph which then may run much more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha, beta = choose_params()\n",
    "%timeit gradient_descent(alpha, beta, x, y, nr_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha, beta = choose_params()\n",
    "quick_descent = tf.function(gradient_descent)\n",
    "%timeit quick_descent(alpha, beta, x, y, nr_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The speed-up is impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise 1: Tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Shorten the following function `get_loss` using `tf.square` or `tf.norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(alpha, beta, x, y):\n",
    "    y_predict = alpha * x + beta\n",
    "    errors = y - y_predict\n",
    "    loss = tf.reduce_sum(errors * errors)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise 2: Computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Make TensorFlow compute the derivative of the function $t \\mapsto \\cos t * \\sin t$ at $t=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise 3: Computing second-order differentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Compute the second derivative of the function $f(t) = t\\cos(t)$ at $t=1$ and check that it equals $\\-cos(1) - 2\\sin(1)$:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "01_linear_regression.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
